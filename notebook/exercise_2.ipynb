{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Live shared task\n",
    "\n",
    "The challenge is to build a sentence-level classifier for identyfing [adverse drug events](https://en.wikipedia.org/wiki/Adverse_event) in 60 minutes. You are free to use any data and annotation strategy you think best trades off hacking and labelling. Just please don't look at the test data.\n",
    "\n",
    "Some strategies to consider:\n",
    "* Get started with random or query-driven sampling.\n",
    "* Use the dev data for seeding learning instead of generalisation testing and analysis.\n",
    "* Tune classifier choice, hyperparameters or feature extraction.\n",
    "* Use error analysis over the dev data to refine your strategy.\n",
    "* Active learning by uncertainty or ensembles.\n",
    "* Collect 10 or more query functions and use as snorkel labelling functions.\n",
    "* Find additional data, e.g., [Twitter](https://archive.org/details/twitterstream).\n",
    "* Interactive web search or [Reddit queries](http://minimaxir.com/2015/10/reddit-bigquery/).\n",
    "* Use external data (e.g., [MAUDE](https://www.fda.gov/MedicalDevices/DeviceRegulationandGuidance/PostmarketRequirements/ReportingAdverseEvents/ucm127891.htm)) for querying or labelling functions.\n",
    "\n",
    "Please don't use data from the following as they are sources of our held-out data:\n",
    "* CSIRO CADEC data set\n",
    "* AskaPatient\n",
    "* DIEGO Lab Twitter data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Labels are saved on the following objects. Only run this once, unless you want delete your annotations and start over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tracking performance\n",
    "batches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset\n",
    "\n",
    "# load dev data\n",
    "dev = Dataset.from_csv('../shared-task/dev.csv')\n",
    "print('Loaded {} items to dev dataset'.format(len(dev)))\n",
    "\n",
    "# get text and label vectors for scikit-learn\n",
    "X_dev, y_dev = zip(*dev.oracle_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unlabelled data pools\n",
    "aska = Dataset.from_csv('../shared-task/aska.csv')\n",
    "print('Loaded {} items to aska dataset'.format(len(aska)))\n",
    "\n",
    "#ader = Dataset.from_csv('../shared-task/ader.csv')\n",
    "#print('Loaded {} items to ader dataset'.format(len(ader)))\n",
    "\n",
    "#adeb = Dataset.from_csv('../shared-task/adeb.csv')\n",
    "#print('Loaded {} items to adeb dataset'.format(len(adeb)))\n",
    "\n",
    "adrc = Dataset.from_csv('../shared-task/adrc.csv')\n",
    "print('Loaded {} items to adrc dataset'.format(len(adrc)))\n",
    "\n",
    "DATASETS = [\n",
    "    ('aska', aska),\n",
    "    #('ader', ader),\n",
    "    #('adeb', adeb),\n",
    "    ('adrc', adrc),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (text, label) in enumerate(dev.oracle_items):\n",
    "    if i > 9:\n",
    "        break\n",
    "    print(i, label, repr(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some unlabelled pool data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (text, label) in enumerate(aska.oracle_items):\n",
    "    if i > 9:\n",
    "        break\n",
    "    print(i, label, repr(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pool data\n",
    "\n",
    "Now let's load the unlabelled pool data. We have data from several sources:\n",
    "* `aska` - Posts for additional drugs from AskaPatient\n",
    "* `ader` - Comments mentioning the same drugs from Reddit\n",
    "* `adeb` - Tweets mentioning the same set of drugs\n",
    "* `adrc` - Tweets mentioning an overlapping set of drugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from samplers import Random\n",
    "import re\n",
    "\n",
    "# set up a random sampler with a query filter that mathces examples containing the word pain\n",
    "def mentions_pain(item):\n",
    "    return bool(re.search(r'\\bpain\\b', item[0], flags=re.IGNORECASE))\n",
    "query_sampler = Random(None, batch_size=10, query=mentions_pain)\n",
    "\n",
    "# sample \n",
    "for i, (text, label) in enumerate(query_sampler(aska)):\n",
    "    print(i, label, repr(text[:80]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotator import AnnotationPane\n",
    "\n",
    "# annotate\n",
    "pane = AnnotationPane(aska, query_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aska.label_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate annotations\n",
    "from dataset import pool_data\n",
    "train = pool_data(DATASETS)\n",
    "print(train.label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "from samplers import Random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')),\n",
    "        ('clf', MultinomialNB(alpha=.01)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit pipeline and save train/dev f1 scores\n",
    "from evaluation import fit_and_score\n",
    "\n",
    "batches.append(list(fit_and_score(pipeline, train, X_dev, y_dev, n=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect batches\n",
    "print('Batches:')\n",
    "for i, batch in enumerate(batches):\n",
    "    train_sizes, train_scores, test_scores = zip(*batch)\n",
    "    print('\\n..batch', i)\n",
    "    print('..train_sizes:', train_sizes)\n",
    "    print('..train_scores:', ['{:.2f}'.format(s) for s in train_scores])\n",
    "    print('..test_scores:', ['{:.2f}'.format(s) for s in test_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import plot_learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = zip(*[zip(*i) for i in batches])\n",
    "plt = plot_learning_curve(train_sizes, train_scores, test_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate annotations\n",
    "from dataset import pool_data\n",
    "train = pool_data(DATASETS)\n",
    "print(train.label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the classifier\n",
    "X_train, y_train = zip(*train.labelled_items)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "import random\n",
    "X_dev = [i[0] for i in dev]\n",
    "random.shuffle(X_dev)\n",
    "y_pred = pipeline.predict(X_dev)\n",
    "predictions = dict(zip(X_dev, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "y_true = [dev.get_oracle_label(t) for t in X_dev]\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some errors\n",
    "errors = filter(lambda i: i[0] != i[1], zip(y_true, y_pred, X_dev))\n",
    "for i, (true, pred, text) in enumerate(errors):\n",
    "    if i > 9:\n",
    "        break\n",
    "    print(i, true, pred, repr(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data programming \n",
    "\n",
    "One view of data programming is that it takes the query functions we used in the previous exercise and uses them for weak supervision. It does this by pooling labelling function output using weighted voting.\n",
    "\n",
    "A simple implementation could use the inter-annotator agreement scripts from exercise 1.1 to weight each labelling function by its average agreement score.\n",
    "\n",
    "In the setting here, where we have dev data, we could also weight each labelling function by its perforamance on the labelled dev data. Of course, this wouldn't work in an annotation setting where we were starting without labelled data.\n",
    "\n",
    "A key difference with `snorkel` is that this approach in the annotation framework does not go on to train the classifier on a continuous voting confidence value.\n",
    "\n",
    "Feel free to experiment with voting, or use `snorkel` directly. If you do plan to use `snorkel`, note that it takes a while to [install](https://github.com/HazyResearch/snorkel#installation). It would be a good idea to run the installation in the background while you start annotating and/or writing labelling functions.\n",
    "\n",
    "Once `snorkel` is installed, the tutorials should help get things up and running. These are in the repo and can also be viewed [on github](https://github.com/HazyResearch/snorkel/tree/master/tutorials/intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short strategy description\n",
    "\n",
    "Before submitting, please summarise:\n",
    "* The hacking/labelling strategy you followed\n",
    "* How do you rate this strategy? Why?\n",
    "\n",
    "__TODO Add your summary right here.__\n",
    "\n",
    "__TODO If you have a list sampling strategies, please include it here.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Submit your annotation and system output for scoring.\n",
    "* Union of annotations across all sets (except dev).\n",
    "* Predict dev\n",
    "* Predict test\n",
    "\n",
    "\n",
    "### Step 1: Set up\n",
    "\n",
    "First, we'll set up a pipeline. Feel free to use a different classifier here if you like.\n",
    "\n",
    "__FIXME fix to_csv!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# use multinomial NB again\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')),\n",
    "        ('clf', MultinomialNB(alpha=.01)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions will be written to a USERNAME directory. This will take USER from your environment by default, but feel free to choose another name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "USERNAME = os.environ.get('USER', 'username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ../submissions/$USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train and predict\n",
    "\n",
    "Now lets collate all annotated data into a `train` dataset; use this to train the classifier; and save predictions for dev and test.\n",
    "\n",
    "__FIXME move this into a function for learning curve use as well!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate annotations\n",
    "from dataset import pool_data\n",
    "train = pool_data(DATASETS)\n",
    "print(train.label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save annotations to csv\n",
    "for k, d in DATASETS:\n",
    "    d.to_csv('../submissions/{}/{}.csv'.format(USERNAME, k))\n",
    "train.to_csv('../submissions/{}/train.csv'.format(USERNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the classifier\n",
    "X_train, y_train = zip(*train.labelled_items)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import label_for_submission\n",
    "\n",
    "# prepare system output for dev data\n",
    "label_for_submission(dev, pipeline, 'dev', USERNAME)\n",
    "\n",
    "# prepare system output for test data\n",
    "test = Dataset.from_csv('../shared-task/test.csv')\n",
    "label_for_submission(test, pipeline, 'test', USERNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Copy notebook and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy your notebook to your submission directory\n",
    "! cp exercise_2.ipynb ../submissions/$USER/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push your submission back to the repo\n",
    "! git add ../submissions/$USER\n",
    "! git commit -m \"Checkpoint $USER\" ../submissions/$USER/\n",
    "! git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
